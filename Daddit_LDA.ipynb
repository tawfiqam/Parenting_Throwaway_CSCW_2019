{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:58:21.465605Z",
     "start_time": "2020-02-19T22:58:21.026957Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "from numpy import array\n",
    "from pprint import pprint\n",
    "import re\n",
    "import scipy.stats as stat\n",
    "from os.path import basename\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "from empath import Empath\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import operator\n",
    "from nltk import stem\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.stem import PorterStemmer\n",
    "import dask\n",
    "import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(filename='lda_model_Parenting.log',format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "\n",
    "def Tokinization(document):\n",
    "    document = \"\".join(document)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(document)\n",
    "    return intermediate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:58:30.457574Z",
     "start_time": "2020-02-19T22:58:24.080083Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_pickle('Parenting_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:59:21.140965Z",
     "start_time": "2020-02-19T22:59:20.740901Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a Daddit Data Frame\n",
    "Daddit = df[df['subreddit']=='daddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:59:52.146016Z",
     "start_time": "2020-02-19T22:59:51.873896Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a Mommit Data Frame\n",
    "Mommit = df[df['subreddit']=='Mommit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T23:00:25.245170Z",
     "start_time": "2020-02-19T23:00:25.239432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the Daddit Cleaned dataframe...\n",
      "**************************************************\n",
      "and for a test...\n",
      "1037029                                  silli day like nice\n",
      "1037030                       hmm mean need scale back silli\n",
      "1037031    boy luck nice move exampl take moment offer pr...\n",
      "1037032                        least day much count unintent\n",
      "1037033                           could probabl get day work\n",
      "Name: clean_body, dtype: object\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "print('reading the Daddit Cleaned dataframe...')\n",
    "print('*' * 50)\n",
    "print('and for a test...')\n",
    "print(Daddit.clean_body.head())\n",
    "print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T23:02:57.898605Z",
     "start_time": "2020-02-19T23:02:50.080382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouping into dcouments\n"
     ]
    }
   ],
   "source": [
    "print('grouping into dcouments')\n",
    "CompleteThread = []\n",
    "CompleteThread = Daddit.groupby('link_id')['clean_body'].apply(list)\n",
    "#running for the rest of the data\n",
    "processed_threads = []\n",
    "#creating threads for each of the users\n",
    "for thread in CompleteThread:\n",
    "    #Preprocessing each of the threads\n",
    "    processed_threads.append(Tokinization(thread))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T23:06:53.141012Z",
     "start_time": "2020-02-19T23:06:30.504227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the documents is...\n",
      "44730\n",
      "**************************************************\n",
      "the first document is...\n",
      "['congratul', 'ee', 'goodbeauti', 'babi', 'congrat']\n",
      "**************************************************\n",
      "buiding the dictionary...\n",
      "babi\n",
      "building the corpus\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n"
     ]
    }
   ],
   "source": [
    "texts = processed_threads\n",
    "print('The length of the documents is...')\n",
    "print(len(texts))\n",
    "print('*' * 50)\n",
    "print('the first document is...')\n",
    "print(texts[0])\n",
    "print('*' * 50)\n",
    "\n",
    "print('buiding the dictionary...')\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('DadditDictionary.dict')\n",
    "print(dictionary[0])\n",
    "\n",
    "print('building the corpus')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('DadditCorpusFinal.mm', corpus)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T23:10:19.580052Z",
     "start_time": "2020-02-19T23:10:19.574512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading all LDA models....\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "print('reading all LDA models....')\n",
    "print('*' * 50)\n",
    "\n",
    "model_list =  []\n",
    "\n",
    "coherence_values = []\n",
    "\n",
    "for i in range(0, len(model_list)):\n",
    "    print('now working on coherence value for model...')\n",
    "    print(model_list[i])\n",
    "    print('*' * 50)\n",
    "    coherencemodel = CoherenceModel(model=model_list[i], texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T23:13:25.999586Z",
     "start_time": "2020-02-19T23:13:25.994367Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T00:04:39.607840Z",
     "start_time": "2020-02-19T23:17:16.223431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tammari/Desktop/DaskTest/env/lib/python3.6/site-packages/scipy/sparse/lil.py:504: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/tammari/Desktop/DaskTest/env/lib/python3.6/site-packages/scipy/sparse/lil.py:506: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=10, limit=100, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T00:09:02.791123Z",
     "start_time": "2020-02-20T00:09:02.589784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(70, 0.4855468327726064), (20, 0.47824322359635907), (40, 0.47508608717363676), (80, 0.47193974476475054), (60, 0.46377945867955206), (50, 0.4605282992545867), (90, 0.45507055464394863), (30, 0.4380515627620837), (10, 0.3842091286860035)]\n",
      "**************************************************\n",
      "Num Topics = 70  has Coherence Value of 0.4855\n",
      "Num Topics = 20  has Coherence Value of 0.4782\n",
      "Num Topics = 40  has Coherence Value of 0.4751\n",
      "Num Topics = 80  has Coherence Value of 0.4719\n",
      "Num Topics = 60  has Coherence Value of 0.4638\n",
      "Num Topics = 50  has Coherence Value of 0.4605\n",
      "Num Topics = 90  has Coherence Value of 0.4551\n",
      "Num Topics = 30  has Coherence Value of 0.4381\n",
      "Num Topics = 10  has Coherence Value of 0.3842\n",
      "saving list...\n",
      "**************************************************\n",
      "creating a graph of the coherence model...\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "limit = 100\n",
    "start = 10\n",
    "step  = 10    \n",
    "\n",
    "x = range(start, limit, step)\n",
    "coherence = zip(x, coherence_values)\n",
    "sorted_coherence = sorted(coherence, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_coherence)\n",
    "print('*' * 50)\n",
    "\n",
    "for m, cv in sorted_coherence:\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    \n",
    "print('saving list...')\n",
    "print('*' * 50)\n",
    "with open(\"daddit_cohere.pkl\", \"wb\") as fp: \n",
    "    pickle.dump(sorted_coherence, fp)\n",
    "    \n",
    "print('creating a graph of the coherence model...')\n",
    "print('*' * 50)\n",
    "\n",
    "# Show graph\n",
    "fig = plt.gcf()\n",
    "limit=limit; start=start; step=step;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "fig.savefig('Daddit_LDA_models_coherence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T00:16:15.222564Z",
     "start_time": "2020-02-20T00:16:09.735396Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(model_list)):\n",
    "    model_list[i].save('Daddit_Model'+str(i)+\".model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
